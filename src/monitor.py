#!/usr/bin/env python3
"""
ç”¨å‹ç½‘ç»œæ¸¯è‚¡ä¸Šå¸‚ç›‘æ§è„šæœ¬
ç›‘æ§æ¸¯äº¤æ‰€æŠ«éœ²æ˜“(HKEXnews)å’ŒAè‚¡å…¬å‘Šï¼Œæ•æ‰å…³é”®ä¸Šå¸‚èŠ‚ç‚¹äº‹ä»¶
"""

import os
import json
import hashlib
import logging
import asyncio
import hmac
import base64
import time
import urllib.parse
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, Dict, List, Set
import re

import requests
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.error import TelegramError

# =============================================================================
# Configuration
# =============================================================================

# Telegram Configuration
TELEGRAM_BOT_TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
TELEGRAM_CHAT_ID = os.getenv("TELEGRAM_CHAT_ID")

# Feishu Configuration
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL")
FEISHU_SIGN_SECRET = os.getenv("FEISHU_SIGN_SECRET")  # å¯é€‰ï¼Œç”¨äºç­¾åæ ¡éªŒ

# Data persistence paths (GitHub Actions compatible)
DATA_DIR = Path(os.getenv("DATA_DIR", "./data"))
SEEN_HASHES_FILE = DATA_DIR / "seen_hashes.json"

# Request timeouts
REQUEST_TIMEOUT = 30
RETRY_ATTEMPTS = 3
RETRY_DELAY = 2

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)


# =============================================================================
# Critical Event Keywords (å¿…é¡»æ•æ‰çš„å…³é”®äº‹ä»¶)
# =============================================================================

CRITICAL_KEYWORDS = {
    # æ¸¯äº¤æ‰€ç›¸å…³
    "prospectus": ["PROSPECTUS", "æ‹›è‚¡è¯´æ˜ä¹¦", "æ‹›è‚¡æ›¸"],
    "global_offering": ["GLOBAL OFFERING", "å…¨çƒå‘å”®", "é…å”®"],
    "price_range": ["PRICE RANGE", "ä»·æ ¼åŒºé—´", "å‘å”®åŒºé—´", "å‘è¡Œä»·"],
    "allocation": ["ALLOCATION RESULT", "é…å”®ç»“æœ", "ä¸­ç­¾ç»“æœ", "å‘å”®ç»“æœ"],
    "h_share_details": ["Hè‚¡", "H SHARE", "å¢ƒå¤–ä¸Šå¸‚", "å‘è¡Œæ•°é‡", "å æ€»è‚¡æœ¬", "è‚¡ä»½"],
}

# æ’é™¤å…³é”®è¯ (åå™ªéŸ³)
EXCLUDE_KEYWORDS = [
    "APPLICATION PROOF",
    "APPLICATION",
    "ç”³è¯·è¡¨æ ¼",
    "è¡¥é€’",
    "æ›´æ­£",
    "å»¶æœŸ",
    "å­£åº¦æŠ¥å‘Š",
    "å¹´åº¦æŠ¥å‘Š",
    "ä¸­æœŸæŠ¥å‘Š",
]

# ç›®æ ‡å…¬å¸åç§°
TARGET_COMPANIES = ["ç”¨å‹", "YONYOU", "Yonyou"]


# =============================================================================
# Persistence Layer (æŒä¹…åŒ–å»é‡)
# =============================================================================

class DedupManager:
    """ç®¡ç†å·²å¤„ç†å…¬å‘Šçš„å“ˆå¸Œå»é‡ç³»ç»Ÿ"""

    def __init__(self, hash_file: Path):
        self.hash_file = hash_file
        self.seen_hashes: Set[str] = self._load_hashes()

    def _load_hashes(self) -> Set[str]:
        """ä»æ–‡ä»¶åŠ è½½å·²å¤„ç†çš„å“ˆå¸Œ"""
        if self.hash_file.exists():
            try:
                data = json.loads(self.hash_file.read_text(encoding="utf-8"))
                return set(data.get("hashes", []))
            except Exception as e:
                logger.warning(f"Failed to load hashes: {e}")
                return set()
        return set()

    def _save_hashes(self):
        """ä¿å­˜å“ˆå¸Œåˆ°æ–‡ä»¶"""
        try:
            self.hash_file.parent.mkdir(parents=True, exist_ok=True)
            self.hash_file.write_text(
                json.dumps({"hashes": list(self.seen_hashes), "last_updated": datetime.now(timezone.utc).isoformat()}, ensure_ascii=False),
                encoding="utf-8"
            )
            logger.debug(f"Saved {len(self.seen_hashes)} hashes")
        except Exception as e:
            logger.error(f"Failed to save hashes: {e}")

    def is_seen(self, item_id: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å¤„ç†"""
        return item_id in self.seen_hashes

    def mark_seen(self, item_id: str):
        """æ ‡è®°ä¸ºå·²å¤„ç†"""
        self.seen_hashes.add(item_id)
        self._save_hashes()

    def generate_hash(self, title: str, date: str, url: str) -> str:
        """ç”Ÿæˆå…¬å‘Šå”¯ä¸€æ ‡è¯†"""
        content = f"{title}|{date}|{url}".encode("utf-8")
        return hashlib.sha256(content).hexdigest()[:16]


# =============================================================================
# HTTP Client with Retry
# =============================================================================

class Fetcher:
    """å¸¦é‡è¯•å’Œè¶…æ—¶çš„HTTPè¯·æ±‚å™¨"""

    @staticmethod
    def get(url: str, headers: Optional[Dict] = None) -> Optional[requests.Response]:
        """GETè¯·æ±‚withé‡è¯•"""
        for attempt in range(RETRY_ATTEMPTS):
            try:
                response = requests.get(
                    url,
                    headers=headers or {},
                    timeout=REQUEST_TIMEOUT
                )
                response.raise_for_status()
                return response
            except requests.RequestException as e:
                logger.warning(f"Request failed (attempt {attempt + 1}/{RETRY_ATTEMPTS}): {e}")
                if attempt == RETRY_ATTEMPTS - 1:
                    logger.error(f"Failed to fetch {url}")
                    return None
        return None


# =============================================================================
# Event Analyzer
# =============================================================================

class EventAnalyzer:
    """åˆ†æå…¬å‘Šæ˜¯å¦ä¸ºå…³é”®äº‹ä»¶"""

    @staticmethod
    def contains_exclude_keywords(text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«æ’é™¤å…³é”®è¯"""
        text_upper = text.upper()
        for keyword in EXCLUDE_KEYWORDS:
            if keyword.upper() in text_upper:
                return True
        return False

    @staticmethod
    def identify_event_type(title: str, description: str = "") -> Optional[str]:
        """è¯†åˆ«äº‹ä»¶ç±»å‹"""
        content = f"{title} {description}".upper()

        for event_type, keywords in CRITICAL_KEYWORDS.items():
            for keyword in keywords:
                if keyword.upper() in content:
                    return event_type

        return None

    @staticmethod
    def extract_advanced_info(text: str) -> Dict[str, str]:
        """æå–é«˜çº§ä¿¡æ¯ï¼ˆHè‚¡æ¯”ä¾‹ã€ä»·æ ¼ç­‰ï¼‰"""
        info = {}

        # æå–ç™¾åˆ†æ¯”ï¼ˆHè‚¡å‘è¡Œæ¯”ä¾‹ï¼‰
        percentage_match = re.search(r"(\d+(?:\.\d+)?)\s*%", text)
        if percentage_match:
            info["percentage"] = percentage_match.group(1)
            try:
                if float(percentage_match.group(1)) >= 15:
                    info["dilution_warning"] = "ç¨€é‡Šé£é™©åé«˜"
            except ValueError:
                pass

        # æ£€æµ‹ä»·æ ¼åŒºé—´
        if any(keyword in text.upper() for keyword in ["PRICE RANGE", "ä»·æ ¼åŒºé—´", "å‘å”®åŒºé—´"]):
            info["valuation_anchor"] = "ä¼°å€¼é”šå·²å‡ºç°"

        return info


# =============================================================================
# HKEXnews Monitor
# =============================================================================

class HKEXMonitor:
    """æ¸¯äº¤æ‰€æŠ«éœ²æ˜“ç›‘æ§"""

    BASE_URL = "https://www.hkexnews.hk"
    # æ¸¯äº¤æ‰€æœç´¢API v2
    SEARCH_API = f"{BASE_URL}/hkex_api/data/get/search"
    # æ–°ä¸Šå¸‚é¡µé¢
    NEW_LISTINGS_URL = f"{BASE_URL}/new-listing"

    def __init__(self, dedup: DedupManager):
        self.dedup = dedup

    def monitor_new_listings(self) -> List[Dict]:
        """ç›‘æ§æ–°ä¸Šå¸‚æ–‡ä»¶"""
        results = []

        # æ–¹æ¡ˆ1: ä½¿ç”¨æ¸¯äº¤æ‰€æœç´¢API
        try:
            logger.info("Trying HKEX search API v2...")
            results = self._fetch_search_api()
            if results:
                return results
        except Exception as e:
            logger.debug(f"Search API failed: {e}")

        # æ–¹æ¡ˆ2: è§£ææ–°ä¸Šå¸‚é¡µé¢HTML
        try:
            logger.info("Trying to parse new listings page...")
            results = self._fetch_new_listings_page()
        except Exception as e:
            logger.error(f"HKEX monitoring error: {e}")

        return results

    def _fetch_search_api(self) -> List[Dict]:
        """ä½¿ç”¨æ¸¯äº¤æ‰€æœç´¢API v2"""
        url = self.SEARCH_API

        # æ¸¯äº¤æ‰€æœç´¢å‚æ•°
        params = {
            "lang": "EN",
            "searchType": "ALL",
            "companyName": "Yonyou",
            "documentType": ["NEW_LISTING", "PROSPECTUS"],
            "size": 50
        }

        response = Fetcher.get(
            url,
            headers={
                "Accept": "application/json",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )
        if not response:
            return []

        try:
            data = response.json()
            return self._parse_search_results(data)
        except Exception as e:
            logger.debug(f"Failed to parse JSON: {e}")
            return []

    def _fetch_new_listings_page(self) -> List[Dict]:
        """è§£ææ–°ä¸Šå¸‚é¡µé¢"""
        url = self.NEW_LISTINGS_URL

        response = Fetcher.get(
            url,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
            }
        )
        if not response:
            return []

        return self._parse_new_listings_html(response.text)

    def _parse_search_results(self, data: dict) -> List[Dict]:
        """è§£ææœç´¢APIç»“æœ"""
        results = []

        # æ¸¯äº¤æ‰€APIå¯èƒ½è¿”å›ä¸åŒçš„ç»“æ„
        if "hits" in data:
            items = data["hits"]
        elif "results" in data:
            items = data["results"]
        elif "data" in data:
            items = data["data"]
        else:
            items = []

        for item in items:
            if isinstance(item, dict):
                # å¤„ç†ä¸åŒçš„å­—æ®µå
                title = item.get("title") or item.get("docTitle") or item.get("header", "")
                date_str = item.get("date") or item.get("publishDate") or item.get("dateTime", "")
                url = item.get("url") or item.get("docLink") or item.get("link", "")

                if title and url:
                    parsed = self._process_item({
                        "title": title,
                        "date": date_str,
                        "url": url
                    })
                    if parsed:
                        results.append(parsed)

        return results

    def _parse_new_listings_html(self, html: str) -> List[Dict]:
        """è§£ææ–°ä¸Šå¸‚é¡µé¢HTML"""
        results = []
        soup = BeautifulSoup(html, "html.parser")

        # æ¸¯äº¤æ‰€æ–°ä¸Šå¸‚é¡µé¢çš„é€‰æ‹©å™¨ï¼ˆéœ€è¦æ ¹æ®å®é™…é¡µé¢è°ƒæ•´ï¼‰
        for row in soup.select("tr, .news-item, .listing-item, [class*='listing'], [class*='news']"):
            title_elem = row.select_one("a[href]")
            if not title_elem:
                continue

            title = title_elem.get_text(strip=True)
            link = title_elem.get("href", "")

            if not link:
                continue

            if link.startswith("/"):
                link = f"{self.BASE_URL}{link}"

            # æŸ¥æ‰¾æ—¥æœŸ
            date_elem = row.select_one(".date, time, [class*='date'], [class*='time']")
            date_str = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime("%Y-%m-%d")

            parsed = self._process_item({
                "title": title,
                "date": date_str,
                "url": link
            })

            if parsed:
                results.append(parsed)

        return results

    def _process_item(self, item: Dict) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªå…¬å‘Šé¡¹"""
        title = item.get("title", "")
        date_str = item.get("date", "")
        url = item.get("url", "")

        # æ£€æŸ¥å…¬å¸åç§°åŒ¹é…
        if not any(company.lower() in title.lower() for company in TARGET_COMPANIES):
            return None

        # æ’é™¤å™ªéŸ³
        if EventAnalyzer.contains_exclude_keywords(title):
            logger.debug(f"Excluded by keywords: {title}")
            return None

        # å»é‡
        item_hash = self.dedup.generate_hash(title, date_str, url)
        if self.dedup.is_seen(item_hash):
            logger.debug(f"Already seen: {title}")
            return None

        # è¯†åˆ«äº‹ä»¶ç±»å‹
        event_type = EventAnalyzer.identify_event_type(title)
        if not event_type:
            logger.debug(f"No critical event matched: {title}")
            return None

        self.dedup.mark_seen(item_hash)

        return {
            "source": "HKEXnews",
            "title": title,
            "date": date_str,
            "url": url,
            "event_type": event_type,
            "importance": "HIGH"
        }


# =============================================================================
# A-Share Announcement Monitor
# =============================================================================

class AShareMonitor:
    """Aè‚¡å…¬å‘Šç›‘æ§ï¼ˆä¸Šäº¤æ‰€/å·¨æ½®èµ„è®¯ï¼‰"""

    def __init__(self, dedup: DedupManager):
        self.dedup = dedup

    def monitor_announcements(self) -> List[Dict]:
        """ç›‘æ§Hè‚¡ç›¸å…³å…¬å‘Š"""
        results = []

        # å·¨æ½®èµ„è®¯æœç´¢URL
        search_urls = [
            "http://www.cninfo.com.cn/new/fulltextSearch?notautosubmit=&keyword=Yonyou",
            "http://www.cninfo.com.cn/new/fulltextSearch?notautosubmit=&keyword=ç”¨å‹ç½‘ç»œ",
        ]

        for url in search_urls:
            try:
                response = Fetcher.get(url, headers={"Accept-Language": "zh-CN,zh;q=0.9"})
                if not response:
                    continue

                items = self._parse_cninfo(response.text)
                results.extend(items)

            except Exception as e:
                logger.error(f"A-share monitoring error for {url}: {e}")

        return results

    def _parse_cninfo(self, html: str) -> List[Dict]:
        """è§£æå·¨æ½®èµ„è®¯å“åº”"""
        results = []
        soup = BeautifulSoup(html, "html.parser")

        for item in soup.select(".result-item, .news-item, tr"):
            title_elem = item.select_one("a[title], .title")
            date_elem = item.select_one(".date, time")
            link_elem = item.select_one("a[href]")

            if not title_elem or not link_elem:
                continue

            title = title_elem.get_text(strip=True) or title_elem.get("title", "").strip()
            date_str = date_elem.get_text(strip=True) if date_elem else datetime.now().strftime("%Y-%m-%d")
            link = link_elem.get("href", "")

            # åªå¤„ç†Hè‚¡ç›¸å…³å…¬å‘Š
            if not any(kw in title.upper() for kw in ["Hè‚¡", "H SHARE", "å¢ƒå¤–ä¸Šå¸‚", "é¦™æ¸¯"]):
                continue

            # æ’é™¤å™ªéŸ³
            if EventAnalyzer.contains_exclude_keywords(title):
                continue

            # å»é‡
            item_hash = self.dedup.generate_hash(title, date_str, link)
            if self.dedup.is_seen(item_hash):
                continue

            # è¯†åˆ«äº‹ä»¶ç±»å‹
            event_type = EventAnalyzer.identify_event_type(title)
            if not event_type:
                continue

            self.dedup.mark_seen(item_hash)

            results.append({
                "source": "CNINFO",
                "title": title,
                "date": date_str,
                "url": link,
                "event_type": event_type,
                "importance": "HIGH"
            })

        return results


# =============================================================================
# Telegram Notifier
# =============================================================================

class TelegramNotifier:
    """Telegramæ¨é€é€šçŸ¥"""

    def __init__(self):
        if not TELEGRAM_BOT_TOKEN or not TELEGRAM_CHAT_ID:
            raise ValueError("TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID must be set")

        self.bot = Bot(token=TELEGRAM_BOT_TOKEN)
        self.chat_id = TELEGRAM_CHAT_ID

    def send_alert(self, event: Dict):
        """å‘é€äº‹ä»¶æé†’"""
        message = self._format_message(event)

        try:
            # ä½¿ç”¨ asyncio è¿è¡Œå¼‚æ­¥æ–¹æ³•
            asyncio.run(self._send_message_async(message))
            logger.info(f"Alert sent: {event['title'][:50]}")
        except Exception as e:
            logger.error(f"Telegram error: {e}")

    async def _send_message_async(self, message: str):
        """å¼‚æ­¥å‘é€æ¶ˆæ¯"""
        await self.bot.send_message(
            chat_id=self.chat_id,
            text=message,
            parse_mode=None,  # ä½¿ç”¨çº¯æ–‡æœ¬ï¼Œé¿å… Markdown è§£æé”™è¯¯
            disable_web_page_preview=True
        )

    def _format_message(self, event: Dict) -> str:
        """æ ¼å¼åŒ–æ¨é€æ¶ˆæ¯"""
        event_type_names = {
            "prospectus": "æ­£å¼æ‹›è‚¡è¯´æ˜ä¹¦ï¼ˆProspectusï¼‰",
            "global_offering": "å…¨çƒå‘å”® / Global Offering",
            "price_range": "ä»·æ ¼åŒºé—´ / Price Range",
            "allocation": "é…å”®ç»“æœ / Allocation Results",
            "h_share_details": "Hè‚¡å‘è¡Œè¯¦æƒ…",
        }

        event_name = event_type_names.get(event["event_type"], event["event_type"])

        message = f"""ã€ç”¨å‹æ¸¯è‚¡ä¸Šå¸‚ Â· å…³é”®è¿›å±•ã€‘
äº‹ä»¶ï¼š{event_name}
æ—¥æœŸï¼š{event['date']}
æ¥æºï¼š{event['source']}
é“¾æ¥ï¼š{event['url']}
é‡è¦æ€§ï¼š{event['importance']}"""

        # æ·»åŠ é«˜çº§ä¿¡æ¯
        advanced_info = EventAnalyzer.extract_advanced_info(event["title"])
        if advanced_info:
            message += "\n\né™„åŠ ä¿¡æ¯ï¼š"
            for key, value in advanced_info.items():
                message += f"\n  â€¢ {value}"

        return message


# =============================================================================
# Feishu Notifier
# =============================================================================

class FeishuNotifier:
    """é£ä¹¦æ¨é€é€šçŸ¥"""

    def __init__(self):
        if not FEISHU_WEBHOOK_URL:
            raise ValueError("FEISHU_WEBHOOK_URL must be set")
        self.webhook_url = FEISHU_WEBHOOK_URL
        self.sign_secret = FEISHU_SIGN_SECRET

    def send_alert(self, event: Dict):
        """å‘é€äº‹ä»¶æé†’"""
        message = self._format_message(event)

        try:
            self._send_message(message)
            logger.info(f"Alert sent to Feishu: {event['title'][:50]}")
        except Exception as e:
            logger.error(f"Feishu error: {e}")

    def _generate_sign(self, timestamp: int) -> Optional[str]:
        """ç”Ÿæˆé£ä¹¦æœºå™¨äººç­¾å

        Args:
            timestamp: å½“å‰æ—¶é—´æˆ³ï¼ˆç§’ï¼‰

        Returns:
            Base64ç¼–ç çš„ç­¾åï¼Œå¦‚æœæœªé…ç½®å¯†é’¥åˆ™è¿”å›None
        """
        if not self.sign_secret:
            return None

        # æ‹¼æ¥ç­¾åå­—ç¬¦ä¸²ï¼štimestamp + "\n" + secret
        string_to_sign = f"{timestamp}\n{self.sign_secret}"

        # ä½¿ç”¨HMAC-SHA256åŠ å¯†
        hmac_code = hmac.new(
            self.sign_secret.encode('utf-8'),
            string_to_sign.encode('utf-8'),
            digestmod=hashlib.sha256
        ).digest()

        # Base64ç¼–ç 
        sign = base64.b64encode(hmac_code).decode('utf-8')

        return sign

    def _send_message(self, message: str):
        """å‘é€æ¶ˆæ¯åˆ°é£ä¹¦"""
        # é£ä¹¦å¡ç‰‡æ¶ˆæ¯æ ¼å¼
        card_content = {
            "msg_type": "interactive",
            "card": {
                "header": {
                    "title": {
                        "tag": "plain_text",
                        "content": "ğŸ“¢ ç”¨å‹æ¸¯è‚¡ä¸Šå¸‚ Â· å…³é”®è¿›å±•"
                    },
                    "template": "blue"
                },
                "elements": [
                    {
                        "tag": "div",
                        "text": {
                            "tag": "lark_md",
                            "content": message
                        }
                    }
                ]
            }
        }

        # å‡†å¤‡è¯·æ±‚URL
        url = self.webhook_url
        if self.sign_secret:
            # ç”Ÿæˆæ—¶é—´æˆ³å’Œç­¾å
            timestamp = int(time.time())
            sign = self._generate_sign(timestamp)

            # å°†ç­¾åå‚æ•°æ·»åŠ åˆ° URL
            params = urllib.parse.urlencode({
                "timestamp": str(timestamp),
                "sign": sign
            })
            url = f"{self.webhook_url}?{params}"
            logger.debug(f"Using signature verification: timestamp={timestamp}")

        response = requests.post(
            url,
            json=card_content,
            timeout=10
        )
        response.raise_for_status()

        # æ£€æŸ¥è¿”å›çŠ¶æ€
        data = response.json()
        if data.get("code") != 0:
            raise Exception(f"Feishu API error: {data.get('msg')}")

    def _format_message(self, event: Dict) -> str:
        """æ ¼å¼åŒ–æ¨é€æ¶ˆæ¯ï¼ˆé£ä¹¦ Markdown æ ¼å¼ï¼‰"""
        event_type_names = {
            "prospectus": "æ­£å¼æ‹›è‚¡è¯´æ˜ä¹¦ï¼ˆProspectusï¼‰",
            "global_offering": "å…¨çƒå‘å”® / Global Offering",
            "price_range": "ä»·æ ¼åŒºé—´ / Price Range",
            "allocation": "é…å”®ç»“æœ / Allocation Results",
            "h_share_details": "Hè‚¡å‘è¡Œè¯¦æƒ…",
        }

        event_name = event_type_names.get(event["event_type"], event["event_type"])

        # ä½¿ç”¨é£ä¹¦ Markdown æ ¼å¼
        message = f"""**äº‹ä»¶ç±»å‹ï¼š** {event_name}
**æ—¥æœŸï¼š** {event['date']}
**æ¥æºï¼š** {event['source']}
**é‡è¦æ€§ï¼š** {event['importance']}

**é“¾æ¥ï¼š** [{event['title']}]({event['url']})"""

        # æ·»åŠ é«˜çº§ä¿¡æ¯
        advanced_info = EventAnalyzer.extract_advanced_info(event["title"])
        if advanced_info:
            message += "\n\n**é™„åŠ ä¿¡æ¯ï¼š**"
            for key, value in advanced_info.items():
                message += f"\nâ€¢ {value}"

        return message


# =============================================================================
# Main Orchestrator
# =============================================================================

def main():
    """ä¸»å‡½æ•°"""
    logger.info("=" * 60)
    logger.info("Yonyou HK Listing Monitor Started")
    logger.info("=" * 60)

    # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼
    test_mode = os.getenv("TEST_MODE", "false").lower() == "true"

    if test_mode:
        logger.info("Running in TEST MODE - sending test notification...")
        test_event = {
            "source": "TEST",
            "title": "ã€æµ‹è¯•ã€‘ç”¨å‹æ¸¯è‚¡ä¸Šå¸‚ç›‘æ§ç³»ç»Ÿ",
            "date": datetime.now().strftime("%Y-%m-%d"),
            "url": "https://github.com/Laokuiyin/yonyou_moniter",
            "event_type": "prospectus",
            "importance": "TEST"
        }

        # å°è¯•ä½¿ç”¨é£ä¹¦
        if FEISHU_WEBHOOK_URL:
            try:
                notifier = FeishuNotifier()
                notifier.send_alert(test_event)
                logger.info("Test notification sent to Feishu!")
            except Exception as e:
                logger.error(f"Feishu test failed: {e}")

        # å°è¯•ä½¿ç”¨ Telegram
        if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
            try:
                notifier = TelegramNotifier()
                notifier.send_alert(test_event)
                logger.info("Test notification sent to Telegram!")
            except Exception as e:
                logger.error(f"Telegram test failed: {e}")

        logger.info("Test completed")
        return

    # åˆå§‹åŒ–å»é‡ç®¡ç†å™¨
    dedup = DedupManager(SEEN_HASHES_FILE)
    logger.info(f"Loaded {len(dedup.seen_hashes)} seen hashes")

    all_events = []

    # æ¸¯äº¤æ‰€ç›‘æ§å·²ç¦ç”¨ï¼ˆéœ€è¦ç”³è¯· API keyï¼‰
    logger.info("HKEXnews monitoring: DISABLED (requires API key registration)")
    logger.info("To enable HKEX monitoring, register at: https://www.hkexnews.hk/")

    # ç›‘æ§Aè‚¡å…¬å‘Š
    logger.info("Monitoring A-share announcements...")
    ashare_monitor = AShareMonitor(dedup)
    ashare_events = ashare_monitor.monitor_announcements()
    logger.info(f"A-share: {len(ashare_events)} new critical events")
    all_events.extend(ashare_events)

    if not all_events:
        logger.info("No new critical events found")
        return

    # å‘é€é€šçŸ¥
    logger.info(f"Sending {len(all_events)} notifications...")

    # ä½¿ç”¨é£ä¹¦å‘é€
    if FEISHU_WEBHOOK_URL:
        try:
            notifier = FeishuNotifier()
            for event in all_events:
                notifier.send_alert(event)
            logger.info(f"Sent {len(all_events)} notifications to Feishu")
        except Exception as e:
            logger.error(f"Feishu notification failed: {e}")

    # ä½¿ç”¨ Telegram å‘é€
    if TELEGRAM_BOT_TOKEN and TELEGRAM_CHAT_ID:
        try:
            notifier = TelegramNotifier()
            for event in all_events:
                notifier.send_alert(event)
            logger.info(f"Sent {len(all_events)} notifications to Telegram")
        except Exception as e:
            logger.error(f"Telegram notification failed: {e}")

    logger.info("Monitoring completed")


if __name__ == "__main__":
    main()
